{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-11T20:40:22.82376Z","iopub.status.busy":"2024-06-11T20:40:22.82344Z","iopub.status.idle":"2024-06-11T20:40:37.514146Z","shell.execute_reply":"2024-06-11T20:40:37.512862Z","shell.execute_reply.started":"2024-06-11T20:40:22.823735Z"},"trusted":true},"outputs":[],"source":["# System & General libraries\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import sys\n","import time\n","from collections import defaultdict\n","\n","# Image Processing & Computer Vision\n","import cv2\n","\n","# Machine Learning & Deep Learning\n","import tensorflow as tf\n","from tensorflow.keras.applications import EfficientNetB4\n","from tensorflow.keras import layers, models\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Data Visualization\n","import plotly.graph_objs as go\n","from plotly.offline import iplot\n","import matplotlib.pyplot as plt\n","plt.rc('font', size=14)\n","plt.rc('axes', labelsize=14, titlesize=14)\n","plt.rc('legend', fontsize=14)\n","plt.rc('xtick', labelsize=10)\n","plt.rc('ytick', labelsize=10)\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import seaborn as sns\n","sns.set_theme(style='whitegrid', palette='viridis', font_scale=1.2)\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:37.516592Z","iopub.status.busy":"2024-06-11T20:40:37.516144Z","iopub.status.idle":"2024-06-11T20:40:37.696908Z","shell.execute_reply":"2024-06-11T20:40:37.695904Z","shell.execute_reply.started":"2024-06-11T20:40:37.516566Z"},"trusted":true},"outputs":[],"source":["def get_data():\n","    return pd.read_csv('../input/deepfake-faces/metadata.csv')\n","\n","meta = get_data()\n","meta.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:37.698505Z","iopub.status.busy":"2024-06-11T20:40:37.698188Z","iopub.status.idle":"2024-06-11T20:40:37.926219Z","shell.execute_reply":"2024-06-11T20:40:37.925396Z","shell.execute_reply.started":"2024-06-11T20:40:37.698479Z"},"trusted":true},"outputs":[],"source":["def summary(df):\n","    summary_df = pd.DataFrame(df.dtypes, columns=['dtypes'])\n","    summary_df['count'] = df.count().values\n","    summary_df['unique'] = df.nunique().values\n","    summary_df['missing#'] = df.isna().sum()\n","    summary_df['missing%'] = df.isna().sum() / len(df)\n","    return summary_df\n","summary(meta).style.background_gradient('Purples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:37.928643Z","iopub.status.busy":"2024-06-11T20:40:37.928225Z","iopub.status.idle":"2024-06-11T20:40:37.973264Z","shell.execute_reply":"2024-06-11T20:40:37.972289Z","shell.execute_reply.started":"2024-06-11T20:40:37.928618Z"},"trusted":true},"outputs":[],"source":["print('Fake Images:', len(meta[meta.label=='FAKE']))\n","print('Real Images:', len(meta[meta.label=='REAL']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:37.974711Z","iopub.status.busy":"2024-06-11T20:40:37.974414Z","iopub.status.idle":"2024-06-11T20:40:38.030322Z","shell.execute_reply":"2024-06-11T20:40:38.029082Z","shell.execute_reply.started":"2024-06-11T20:40:37.974688Z"},"trusted":true},"outputs":[],"source":["real_df = meta[meta['label'] == 'REAL']\n","fake_df = meta[meta['label'] == 'FAKE']\n","sample_size = 16000\n","\n","real_df = real_df.sample(sample_size, random_state=42)\n","fake_df = fake_df.sample(sample_size, random_state=42)\n","\n","sample_meta = pd.concat([real_df, fake_df])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:38.032203Z","iopub.status.busy":"2024-06-11T20:40:38.03151Z","iopub.status.idle":"2024-06-11T20:40:38.137576Z","shell.execute_reply":"2024-06-11T20:40:38.136564Z","shell.execute_reply.started":"2024-06-11T20:40:38.032174Z"},"trusted":true},"outputs":[],"source":["Train_set, Test_set = train_test_split(sample_meta, test_size=0.2, random_state=42, stratify=sample_meta['label'])\n","Train_set, Val_set = train_test_split(Train_set, test_size=0.3, random_state=42, stratify=Train_set['label'])\n","\n","print(f'Train Set: {Train_set.shape}')\n","print(f'Validation Set: {Val_set.shape}')\n","print(f'Test Set: {Test_set.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:38.139235Z","iopub.status.busy":"2024-06-11T20:40:38.138879Z","iopub.status.idle":"2024-06-11T20:40:39.818632Z","shell.execute_reply":"2024-06-11T20:40:39.817761Z","shell.execute_reply.started":"2024-06-11T20:40:38.139207Z"},"trusted":true},"outputs":[],"source":["def plot_class_counts(train_set, val_set, test_set):\n","    sets = ['Train Set', 'Validation Set', 'Test Set']\n","    colors = ['#52A666', '#C15B4E']\n","    \n","    y = {\n","        'REAL': [np.sum(train_set == 'REAL'), np.sum(val_set == 'REAL'), np.sum(test_set == 'REAL')],\n","        'FAKE': [np.sum(train_set == 'FAKE'), np.sum(val_set == 'FAKE'), np.sum(test_set == 'FAKE')]\n","    }\n","    \n","    trace0 = go.Bar(x=sets, y=y['REAL'], name='REAL', marker={'color': colors[0]}, opacity=0.7)\n","    trace1 = go.Bar(x=sets, y=y['FAKE'], name='FAKE', marker={'color': colors[1]}, opacity=0.7)\n","    \n","    data = [trace0, trace1]\n","    layout = go.Layout(title='Count of Classes in each set:', xaxis={'title': 'Set'}, yaxis={'title': 'Count'})\n","    \n","    fig = go.Figure(data, layout)\n","    iplot(fig)\n","    \n","plot_class_counts(np.array(Train_set['label']), np.array(Val_set['label']), np.array(Test_set['label']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:39.820593Z","iopub.status.busy":"2024-06-11T20:40:39.820212Z","iopub.status.idle":"2024-06-11T20:40:49.236991Z","shell.execute_reply":"2024-06-11T20:40:49.235548Z","shell.execute_reply.started":"2024-06-11T20:40:39.820559Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15,15), dpi=300)\n","for idx,i in enumerate(Train_set.index[75:100]):\n","    plt.subplot(5,5,idx+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    img = cv2.imread('../input/deepfake-faces/faces_224/'+Train_set.loc[i,'videoname'][:-4]+'.jpg')\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.imshow(img)\n","    \n","    plt.xlabel('FAKE Image' if Train_set.loc[i,'label']=='FAKE' else 'REAL Image')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:49.239301Z","iopub.status.busy":"2024-06-11T20:40:49.238851Z","iopub.status.idle":"2024-06-11T20:40:49.246277Z","shell.execute_reply":"2024-06-11T20:40:49.24528Z","shell.execute_reply.started":"2024-06-11T20:40:49.239262Z"},"trusted":true},"outputs":[],"source":["# Function for getting image paths and corresponding labels from set\n","def retrieve_dataset(set_name):\n","    images, labels = [], []\n","    for (img, imgclass) in zip(set_name['videoname'], set_name['label']):\n","        images.append(cv2.imread('../input/deepfake-faces/faces_224/'+img[:-4]+'.jpg'))\n","        labels.append(1 if imgclass == 'FAKE' else 0)\n","    return np.array(images), np.array(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T20:40:49.251185Z","iopub.status.busy":"2024-06-11T20:40:49.250831Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","X_train, y_train = retrieve_dataset(Train_set)\n","X_val, y_val = retrieve_dataset(Val_set)\n","X_test, y_test = retrieve_dataset(Test_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","tf.keras.backend.clear_session() \n","tf.random.set_seed(42) \n","\n","batch_size = 16\n","preprocess = tf.keras.applications.efficientnet.preprocess_input  \n","\n","train_set_raw = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","valid_set_raw = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n","test_set_raw = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","\n","train_set = train_set_raw.map(lambda X, y: (preprocess(tf.cast(X, tf.float32)), y))\n","train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n","valid_set = valid_set_raw.map(lambda X, y: (preprocess(tf.cast(X, tf.float32)), y)).batch(batch_size)\n","test_set = test_set_raw.map(lambda X, y: (preprocess(tf.cast(X, tf.float32)), y)).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.applications import EfficientNetB4\n","\n","# Build EfficientNetB4 model\n","base_model = EfficientNetB4(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n","\n","# Internal Data Augmentation Layer\n","data_augmentation = tf.keras.Sequential([\n","    tf.keras.layers.RandomFlip(\"horizontal\"),\n","    tf.keras.layers.RandomRotation(0.1),\n","    tf.keras.layers.RandomZoom(0.1),\n","])\n","\n","# Apply Data Augmentation\n","inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n","x = data_augmentation(inputs)\n","x = base_model(x, training=False) # using base model in inference mode\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = BatchNormalization()(x)\n","x = Dense(1024, activation=\"relu\")(x)\n","x = Dropout(0.3)(x)\n","x = Dense(516, activation=\"relu\")(x)\n","x = Dropout(0.3)(x)\n","x = Dense(256, activation=\"relu\")(x)\n","x = Dropout(0.3)(x)\n","x = Dense(128, activation=\"relu\")(x)\n","x = Dropout(0.3)(x)\n","outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","model = tf.keras.Model(inputs, outputs)\n","\n","# Compile the model\n","optimizer = Adam(learning_rate=0.0001)  \n","model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Define the callbacks\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=25,  \n","    restore_best_weights=True,  \n","    verbose=1\n",")\n","\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    'check1.keras', \n","    monitor='val_accuracy',  \n","    save_best_only=True,  \n","    verbose=1\n",")\n","\n","history = model.fit(\n","    train_set, \n","    validation_data=valid_set, \n","    epochs=40,\n","    callbacks=[early_stopping, model_checkpoint] \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Visualize training history\n","sns.set(style=\"whitegrid\")\n","plt.figure(figsize=(12, 6))\n","\n","# Plot training & validation accuracy values\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","# Plot training & validation loss values\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"\\nTest Loss: {test_loss:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Confusion matrix\n","y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n","conf_mat = confusion_matrix(y_test, y_pred)\n","\n","# Confusion matrix\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 16})\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save('deepfake_model.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_image(image_path):\n","    img = cv2.imread(image_path)\n","    img = cv2.resize(img, (224, 224))\n","    img = tf.keras.applications.efficientnet.preprocess_input(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_image_paths = []\n","for idx,i in enumerate(Test_set.index[125:150]):\n","    test_image_paths.append(('../input/deepfake-faces/faces_224/'+Test_set.loc[i,'videoname'][:-4]+'.jpg', Test_set.loc[i,'label']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15,15), dpi=300)\n","\n","for idx, (image_path, label) in enumerate(test_image_paths):\n","    processed_image = preprocess_image(image_path)\n","    processed_image = np.expand_dims(processed_image, axis=0)\n","    prediction = model.predict(processed_image)\n","    \n","    predicted_class = \"FAKE\" if prediction[0, 0] > 0.5 else \"REAL\"\n","    \n","    plt.subplot(5,5,idx+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    \n","    img = cv2.imread(image_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.imshow(img)\n","    \n","    plt.xlabel(f'{predicted_class} | {prediction[0, 0]:.2f} | {\"T\" if predicted_class == label else \"F\"}')\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import tensorflow as tf\n","\n","# Function to preprocess the image\n","def preprocess_image(image_path):\n","    img = cv2.imread(image_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(img, (224, 224))  \n","    img = tf.keras.applications.efficientnet.preprocess_input(img)\n","    return img\n","\n","plt.figure(figsize=(15, 15), dpi=300)\n","\n","# Iterate over a subset of the training set\n","for idx, i in enumerate(Train_set.index[75:100]):\n","    plt.subplot(5, 5, idx + 1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    \n","    image_path = f\"../input/deepfake-faces/faces_224/{Train_set.loc[i, 'videoname'][:-4]}.jpg\"\n","    label = Train_set.loc[i, 'label']\n","    \n","    # Preprocess the image and make a prediction\n","    processed_image = preprocess_image(image_path)\n","    processed_image = np.expand_dims(processed_image, axis=0)\n","    prediction = model.predict(processed_image)\n","    \n","    # Threshold for binary classification: 0.33\n","    predicted_class = \"FAKE\" if prediction[0, 0] > 0.33 else \"REAL\"\n","    \n","    # Load and display the image\n","    img = cv2.imread(image_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.imshow(img)\n","    \n","    true_class = \"FAKE\" if label == 'FAKE' else \"REAL\"\n","    correctness = \"T\" if predicted_class == true_class else \"F\"\n","    plt.xlabel(f'{predicted_class} | {prediction[0, 0]:.2f} | {correctness}')\n","    \n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install tensorflow-docs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow_docs.vis import embed\n","from tensorflow import keras\n","#from imutils import paths\n","from tensorflow_docs.vis import embed\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import imageio\n","import cv2\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DATA_FOLDER = '../input/deepfake-detection-challenge'\n","TRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n","TEST_FOLDER = 'test_videos'\n","\n","print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\n","print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n","train_sample_metadata.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sample_metadata.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].sample(3).index)\n","fake_train_sample_video"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def display_image_from_video(video_path):\n","    capture_image = cv2.VideoCapture(video_path) \n","    ret, frame = capture_image.read()\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(111)\n","    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    ax.imshow(frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for video_file in fake_train_sample_video:\n","    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["real_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='REAL'].sample(3).index)\n","real_train_sample_video"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for video_file in real_train_sample_video:\n","    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sample_metadata['original'].value_counts()[0:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):\n","    plt.figure()\n","    fig, ax = plt.subplots(2,3,figsize=(16,8))\n","    # we only show images extracted from the first 6 videos\n","    for i, video_file in enumerate(video_path_list[0:6]):\n","        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)\n","        capture_image = cv2.VideoCapture(video_path) \n","        ret, frame = capture_image.read()\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        ax[i//3, i%3].imshow(frame)\n","        ax[i//3, i%3].set_title(f\"Video: {video_file}\")\n","        ax[i//3, i%3].axis('on')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["same_original_fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.original=='atvmxvwyns.mp4'].index)\n","display_image_from_video_list(same_original_fake_train_sample_video)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_videos.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[2].video))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fake_videos = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].index)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import HTML\n","from base64 import b64encode\n","\n","def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n","    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n","    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n","    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)\n","\n","play_video(fake_videos[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 32\n","EPOCHS = 10\n","\n","MAX_SEQ_LENGTH = 20\n","NUM_FEATURES = 2048"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def crop_center_square(frame):\n","    y, x = frame.shape[0:2]\n","    min_dim = min(y, x)\n","    start_x = (x // 2) - (min_dim // 2)\n","    start_y = (y // 2) - (min_dim // 2)\n","    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n","\n","\n","def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","    try:\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = crop_center_square(frame)\n","            frame = cv2.resize(frame, resize)\n","            frame = frame[:, :, [2, 1, 0]]\n","            frames.append(frame)\n","\n","            if len(frames) == max_frames:\n","                break\n","    finally:\n","        cap.release()\n","    return np.array(frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model_path = '/kaggle/working/deepfake_model.keras'\n","# custom_model = tf.keras.models.load_model(model_path)\n","\n","from tensorflow.keras.applications import EfficientNetB4\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","\n","efficientnet_model = EfficientNetB4(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')\n","\n","def build_feature_extractor(efficientnet_model):\n","    inputs = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n","    x = preprocess_input(inputs)\n","    outputs = efficientnet_model(x)\n","    return tf.keras.Model(inputs, outputs, name=\"feature_extractor\")\n","\n","feature_extractor = build_feature_extractor(efficientnet_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_all_videos(df, root_dir):\n","    num_samples = len(df)\n","    video_paths = list(df.index)\n","    labels = df[\"label\"].values\n","    labels = np.array(labels == 'FAKE').astype(int)\n","\n","\n","    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n","    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n","    # masked with padding or not.\n","    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n","    frame_features = np.zeros(\n","        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","    )\n","\n","    # For each video.\n","    for idx, path in enumerate(video_paths):\n","        # Gather all its frames and add a batch dimension.\n","        frames = load_video(os.path.join(root_dir, path))\n","        frames = frames[None, ...]\n","\n","        # Initialize placeholders to store the masks and features of the current video.\n","        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","        temp_frame_features = np.zeros(\n","            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","        )\n","\n","        # Extract features from the frames of the current video.\n","        for i, batch in enumerate(frames):\n","            video_length = batch.shape[0]\n","            length = min(MAX_SEQ_LENGTH, video_length)\n","            for j in range(length):\n","                temp_frame_features[i, j, :] = feature_extractor.predict(\n","                    batch[None, j, :]\n","                )\n","            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n","\n","        frame_features[idx,] = temp_frame_features.squeeze()\n","        frame_masks[idx,] = temp_frame_mask.squeeze()\n","\n","    return (frame_features, frame_masks), labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","Train_set, Test_set = train_test_split(train_sample_metadata,test_size=0.1,random_state=42,stratify=train_sample_metadata['label'])\n","\n","print(Train_set.shape, Test_set.shape )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data, train_labels = prepare_all_videos(Train_set, \"train\")\n","test_data, test_labels = prepare_all_videos(Test_set, \"test\")\n","\n","# Assuming train_data is a tuple with two elements: (frame_features, frame_masks)\n","frame_features_shape = train_data[0].shape if train_data and len(train_data) > 0 else None\n","frame_masks_shape = train_data[1].shape if train_data and len(train_data) > 1 else None\n","\n","print(f\"Frame features in train set: {frame_features_shape}\")\n","print(f\"Frame masks in train set: {frame_masks_shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"Train data 0 shape: {train_data[0].shape}\")\n","print(f\"Train data 1 shape: {train_data[1].shape}\")\n","print(f\"Train labels shape: {train_labels.shape}\")\n","print(f\"Test data 0 shape: {test_data[0].shape}\")\n","print(f\"Test data 1 shape: {test_data[1].shape}\")\n","print(f\"Test labels shape: {test_labels.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n","mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n","\n","# Build the model\n","x = keras.layers.GRU(16, return_sequences=True, use_cudnn=False)(frame_features_input, mask=mask_input)\n","x = keras.layers.GRU(8, use_cudnn=False)(x)\n","x = keras.layers.Dropout(0.4)(x)\n","x = keras.layers.Dense(8, activation=\"relu\")(x)\n","output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","\n","model = keras.Model([frame_features_input, mask_input], output)\n","\n","# Compile the model\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","# Display model summary\n","model.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint = keras.callbacks.ModelCheckpoint('check.weights.h5', save_weights_only=True, save_best_only=True)\n","history = model.fit(\n","        [train_data[0], train_data[1]],\n","        train_labels,\n","        validation_data=([test_data[0], test_data[1]],test_labels),\n","        callbacks=[checkpoint],\n","        epochs=10,\n","        batch_size=2\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_single_video(frames):\n","    frames = frames[None, ...]\n","    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n","\n","    for i, batch in enumerate(frames):\n","        video_length = batch.shape[0]\n","        length = min(MAX_SEQ_LENGTH, video_length)\n","        for j in range(length):\n","            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n","        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n","\n","    return frame_features, frame_mask\n","\n","def sequence_prediction(path):\n","    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))\n","    frame_features, frame_mask = prepare_single_video(frames)\n","    return model.predict([frame_features, frame_mask])[0]\n","    \n","\n","def to_gif(images):\n","    converted_images = images.astype(np.uint8)\n","    imageio.mimsave(\"animation.gif\", converted_images, fps=30)\n","    return embed.embed_file(\"animation.gif\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_video = \"/kaggle/input/deepfake-detection-challenge/train_sample_videos/aelfnikyqj.mp4\"\n","print(f\"Test video path: {test_video}\")\n","\n","if(sequence_prediction(test_video)<=0.5):\n","    print(f'The predicted class of the video is FAKE')\n","else:\n","    print(f'The predicted class of the video is REAL')\n","\n","play_video(test_video,TEST_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n","print(f\"Test video path: {test_video}\")\n","\n","if(sequence_prediction(test_video)<=0.5):\n","    print(f'The predicted class of the video is FAKE')\n","else:\n","    print(f'The predicted class of the video is REAL')\n","\n","play_video(test_video,TEST_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n","print(f\"Test video path: {test_video}\")\n","\n","if(sequence_prediction(test_video)<=0.5):\n","    print(f'The predicted class of the video is FAKE')\n","else:\n","    print(f'The predicted class of the video is REAL')\n","\n","play_video(test_video,TEST_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_video = \"/kaggle/input/deepfake-detection-challenge/test_videos/aassnaulhq.mp4\"\n","print(f\"Test video path: {test_video}\")\n","\n","if(sequence_prediction(test_video)<=0.5):\n","    print(f'The predicted class of the video is FAKE')\n","else:\n","    print(f'The predicted class of the video is REAL')\n","\n","play_video(test_video,TEST_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n","import numpy as np\n","\n","def build_model():\n","    from keras.layers import Bidirectional, BatchNormalization\n","\n","    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n","    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n","\n","    # Increase model complexity\n","    x = Bidirectional(keras.layers.GRU(128, return_sequences=True, use_cudnn=False))(frame_features_input, mask=mask_input)\n","    x = BatchNormalization()(x)\n","    x = Bidirectional(keras.layers.GRU(64, use_cudnn=False))(x)\n","    x = keras.layers.Dropout(0.5)(x)  # Increased dropout rate\n","\n","    # Add more dense layers\n","    x = keras.layers.Dense(64, activation=\"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = keras.layers.Dense(32, activation=\"relu\")(x)\n","\n","    # Output layer\n","    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","\n","    # Model definition\n","    model = keras.Model([frame_features_input, mask_input], output)\n","\n","    # Compile the model with a modified optimizer\n","    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n","    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","\n","    # Model summary\n","    return model\n","\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * np.exp(-0.1)\n","\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","all_features = np.random.randn(100, MAX_SEQ_LENGTH, NUM_FEATURES)  \n","all_labels = np.random.randint(2, size=(100, 1))                  \n","\n","fold_no = 1\n","for train_index, test_index in kf.split(all_features):\n","    train_features, test_features = all_features[train_index], all_features[test_index]\n","    train_labels, test_labels = all_labels[train_index], all_labels[test_index]\n","    \n","    train_mask = np.ones((train_features.shape[0], MAX_SEQ_LENGTH), dtype=bool)\n","    test_mask = np.ones((test_features.shape[0], MAX_SEQ_LENGTH), dtype=bool)\n","    \n","    model = build_model()\n","\n","    lr_scheduler = LearningRateScheduler(scheduler)\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n","\n","    print(f'Training for fold {fold_no}...')\n","    history = model.fit(\n","        [train_features, train_mask], \n","        train_labels,\n","        validation_data=([test_features, test_mask], test_labels),\n","        epochs=30,\n","        batch_size=10,\n","        callbacks=[lr_scheduler, early_stopping]\n","    )\n","    \n","    fold_no += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_video = \"/kaggle/input/deepfake-detection-challenge/train_sample_videos/aagfhgtpmv.mp4\"\n","print(f\"Test video path: {test_video}\")\n","\n","if(sequence_prediction(test_video)<=0.5):\n","    print(f'The predicted class of the video is FAKE')\n","else:\n","    print(f'The predicted class of the video is REAL')\n","\n","play_video(test_video,TEST_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_video = \"/kaggle/input/deepfake-detection-challenge/train_sample_videos/aknbdpmgua.mp4\"\n","print(f\"Test video path: {test_video}\")\n","\n","if(sequence_prediction(test_video)<=0.5):\n","    print(f'The predicted class of the video is FAKE')\n","else:\n","    print(f'The predicted class of the video is REAL')\n","\n","play_video(test_video,TEST_FOLDER)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":858837,"sourceId":16880,"sourceType":"competition"},{"datasetId":464091,"sourceId":924245,"sourceType":"datasetVersion"},{"datasetId":5187236,"sourceId":8658348,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
